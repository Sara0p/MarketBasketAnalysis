# -*- coding: utf-8 -*-
"""Market Basket .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1V394dmEBS6Cf0Ucb16QW34KI9Iuw9HSI
"""

import pandas as pd
import numpy as np
from google.colab import drive


drive.mount('/content/drive')
# transform it into dataframe
data=pd.read_csv('/content/drive/MyDrive/data_set_aa/marketdata.csv')
## observing the shape of the data
print(data.shape)
data.head()

data.isnull().sum()

#print(missing)
#missing =

data = data.drop('BillNo', axis=1)
data = data.drop('Quantity', axis=1)
data = data.drop('Price', axis=1)
#data = data.drop('Price', axis=1)

data.info(verbose=True)
data.describe()

# drop duplicate
data.drop_duplicates(inplace=True)

# Assume data is your DataFrame
data['Date'] = pd.to_datetime(data['Date'], format='%d.%m.%Y %H:%M', errors='coerce')

# Check for any NaT values in the 'Date' column to handle parsing issues
if data['Date'].isnull().any():
    print("There were issues with some date entries. Check for NaT values.")

# Add 'year', 'month', 'day', and 'day_name' columns
data['year'] = data['Date'].dt.year
data['month'] = data['Date'].dt.month
data['day'] = data['Date'].dt.day
data['day_name'] = data['Date'].dt.day_name()

# Save the DataFrame to a CSV file named 'cleaned_data.csv' in your Google Drive
data.to_csv('/content/drive/MyDrive/task2/cleaned_data.csv', index=False)

print(data.head())

import matplotlib.pyplot as plt  # Import matplotlib.pyplot
import seaborn as sns

plt.figure(figsize=(12,6))

sns.barplot(x = data['Itemname'].value_counts()[:10].index,
           y = data['Itemname'].value_counts()[:10].values)
plt.xticks(size=12, rotation=45)
plt.title('10 Best Selling Items')
plt.show()

# Sample 40% from the dataset
sample_size = int(len(data) * 0.4)  # Calculate 40% of the dataset
sampled_data = data.sample(n=sample_size, random_state=1)  # Random sample

# Display the sampled data
print(sampled_data)

item_count = sampled_data.groupby(['CustomerID', 'Date', 'Itemname'])['Itemname'].count().reset_index(name='Count')
item_count.head(10)

item_count['Transaction'] = item_count.groupby(['CustomerID', 'Date'])['Date'].ngroup()+1

item_count_pivot = item_count.pivot_table(index='Transaction', columns='Itemname', values='Count', aggfunc='sum').fillna(0)

# drop comma
item_count_pivot = item_count_pivot.astype('int32')
#print(item_count_pivot)

from mlxtend.frequent_patterns import association_rules, apriori
basket = item_count.groupby('Transaction')['Itemname'].apply(list).reset_index()
basket_encoded = basket['Itemname'].str.join('|').str.get_dummies('|')
basket_encoded.head()

sample_size = min(100000, basket_encoded.shape[0])
spilt_data = basket_encoded.sample(n=sample_size, random_state=1)

frequent_itemsets = apriori(spilt_data,min_support=0.01, use_colnames=True)
frequent_itemsets = frequent_itemsets.sort_values('support', ascending=False).head(10)

rules = association_rules(frequent_itemsets, metric='lift', min_threshold=0.5)
rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']].sort_values('confidence', ascending=False).head(10)

from mlxtend.frequent_patterns import association_rules, apriori


# الآن طبق خوارزمية Apriori
frequent_itemsets = apriori(basket, min_support=0.05, use_colnames=True)
frequent_itemsets.sort_values('support', ascending=False).head(10)
# استخراج القواعد الارتباطية
rules = association_rules(frequent_itemsets, metric="confidence", min_threshold=0.6)

# عرض القواعد
print(rules)

# إنشاء جدول محوري
basket =item_count.pivot_table(index='CustomerID', columns='Itemname', values='Count', aggfunc='sum', fill_value=0)

# تحويل القيم إلى 0 أو 1
basket = (basket > 0).astype(int)

# عرض الجدول
basket.head()

#rules.head()

#print(frequent_itemsets)
# To access all column names, use the 'columns' attribute:
#frequent_itemsets.columns
# To select multiple columns, use a list of column names:
#frequent_itemsets[['support', 'itemsets']]